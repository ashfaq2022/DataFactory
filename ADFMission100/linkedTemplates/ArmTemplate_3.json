{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "ADFMission100"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/df_RunningTotal')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_Mission100DataLake_productcsv",
								"type": "DatasetReference"
							},
							"name": "ProductCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_SrcDatalake_ADW",
								"type": "DatasetReference"
							},
							"name": "ProductSink"
						}
					],
					"transformations": [
						{
							"name": "window1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ProductID as string,",
						"          Name as string,",
						"          ProductNumber as string,",
						"          Color as string,",
						"          StandardCost as string,",
						"          ListPrice as string,",
						"          Size as string,",
						"          Weight as string,",
						"          ProductCategoryID as string,",
						"          ProductModelID as string,",
						"          SellStartDate as string,",
						"          SellEndDate as string,",
						"          DiscontinuedDate as string,",
						"          ThumbNailPhoto as string,",
						"          ThumbnailPhotoFileName as string,",
						"          rowguid as string,",
						"          ModifiedDate as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> ProductCSV",
						"ProductCSV window(asc(ProductID, true),",
						"     RollOverSum = sum(toDecimal(ListPrice))) ~> window1",
						"window1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          Column_1 as string,",
						"          Column_2 as string,",
						"          Column_3 as string,",
						"          Column_4 as string,",
						"          Column_5 as string,",
						"          Column_6 as string,",
						"          Column_7 as string,",
						"          Column_8 as string,",
						"          Column_9 as string,",
						"          Column_10 as string,",
						"          Column_11 as string,",
						"          Column_12 as string,",
						"          Column_13 as string,",
						"          Column_14 as string,",
						"          Column_15 as string,",
						"          Column_16 as string,",
						"          Column_17 as string,",
						"          Column_18 as string",
						"     ),",
						"     partitionFileNames:['ProductRunningTotal.csv'],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> ProductSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_SCD_Type_1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_customer",
								"type": "DatasetReference"
							},
							"name": "CustomerCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_PractiseDB_Customer",
								"type": "DatasetReference"
							},
							"name": "PractisseDB"
						}
					],
					"transformations": [
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          FirstName as string,",
						"          LastName as string,",
						"          CompanyName as string,",
						"          SalesPerson as string,",
						"          EmailAddress as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> CustomerCSV",
						"CustomerCSV alterRow(upsertIf(true())) ~> alterRow1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          CustomerID as integer,",
						"          FirstName as string,",
						"          LastName as string,",
						"          CompanyName as string,",
						"          SalesPerson as string,",
						"          EmailAddress as string,",
						"          IsActive as boolean,",
						"          EffectiveStartDate as timestamp,",
						"          EndDate as timestamp,",
						"          CreatedDate as timestamp",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:false,",
						"     upsertable:true,",
						"     keys:['FirstName','LastName'],",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          FirstName,",
						"          LastName,",
						"          CompanyName,",
						"          SalesPerson,",
						"          EmailAddress",
						"     )) ~> PractisseDB"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_SCD_Type_2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_customer",
								"type": "DatasetReference"
							},
							"name": "CustomerInputFile"
						},
						{
							"dataset": {
								"referenceName": "ds_PractiseDB_Customer",
								"type": "DatasetReference"
							},
							"name": "PractoseDBCustomer"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_PractiseDB_Customer",
								"type": "DatasetReference"
							},
							"name": "PractiseCustomerTable"
						},
						{
							"dataset": {
								"referenceName": "ds_PractiseDB_Customer",
								"type": "DatasetReference"
							},
							"name": "PractiseCustomerTableExisting"
						}
					],
					"transformations": [
						{
							"name": "GenerateFileHash"
						},
						{
							"name": "GenerateTableHash"
						},
						{
							"name": "CheckIfHaskKeyexists"
						},
						{
							"name": "AddIsActive"
						},
						{
							"name": "select1"
						},
						{
							"name": "CheckTblrecordVsSelect1"
						},
						{
							"name": "AddIsActive0"
						},
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          FirstName as string,",
						"          LastName as string,",
						"          CompanyName as string,",
						"          SalesPerson as string,",
						"          EmailAddress as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> CustomerInputFile",
						"source(output(",
						"          CustomerID as integer,",
						"          FirstName as string,",
						"          LastName as string,",
						"          CompanyName as string,",
						"          SalesPerson as string,",
						"          EmailAddress as string,",
						"          IsActive as boolean,",
						"          EffectiveStartDate as timestamp,",
						"          EndDate as timestamp,",
						"          CreatedDate as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> PractoseDBCustomer",
						"CustomerInputFile derive(HashKey = md5(FirstName,LastName,CompanyName,SalesPerson,EmailAddress)) ~> GenerateFileHash",
						"PractoseDBCustomer derive(HashKey = md5(FirstName,LastName,CompanyName,SalesPerson,EmailAddress)) ~> GenerateTableHash",
						"GenerateFileHash, GenerateTableHash exists(GenerateFileHash@HashKey == GenerateTableHash@HashKey,",
						"     negate:true,",
						"     broadcast: 'auto')~> CheckIfHaskKeyexists",
						"CheckIfHaskKeyexists derive(NewIsActive = 1,",
						"          StartDate = currentDate()) ~> AddIsActive",
						"AddIsActive select(mapColumn(",
						"          FirstName,",
						"          LastName,",
						"          CompanyName,",
						"          SalesPerson,",
						"          EmailAddress,",
						"          HashKey,",
						"          NewIsActive,",
						"          StartDate",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"GenerateTableHash, select1 exists(PractoseDBCustomer@FirstName == select1@FirstName",
						"     && PractoseDBCustomer@LastName == select1@LastName,",
						"     negate:false,",
						"     broadcast: 'auto')~> CheckTblrecordVsSelect1",
						"CheckTblrecordVsSelect1 derive(IsActive = 0,",
						"          EndDate = currentDate()) ~> AddIsActive0",
						"AddIsActive0 alterRow(updateIf(true())) ~> alterRow1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          CustomerID as integer,",
						"          FirstName as string,",
						"          LastName as string,",
						"          CompanyName as string,",
						"          SalesPerson as string,",
						"          EmailAddress as string,",
						"          IsActive as boolean,",
						"          EffectiveStartDate as timestamp,",
						"          EndDate as timestamp,",
						"          CreatedDate as timestamp",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          FirstName,",
						"          LastName,",
						"          CompanyName,",
						"          SalesPerson,",
						"          EmailAddress,",
						"          IsActive = NewIsActive,",
						"          EffectiveStartDate = StartDate",
						"     )) ~> PractiseCustomerTable",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          CustomerID as integer,",
						"          FirstName as string,",
						"          LastName as string,",
						"          CompanyName as string,",
						"          SalesPerson as string,",
						"          EmailAddress as string,",
						"          IsActive as boolean,",
						"          EffectiveStartDate as timestamp,",
						"          EndDate as timestamp,",
						"          CreatedDate as timestamp",
						"     ),",
						"     deletable:false,",
						"     insertable:false,",
						"     updateable:true,",
						"     upsertable:false,",
						"     keys:['FirstName','LastName','CustomerID'],",
						"     skipKeyWrites:true,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          CustomerID,",
						"          FirstName,",
						"          LastName,",
						"          CompanyName,",
						"          SalesPerson,",
						"          EmailAddress,",
						"          IsActive,",
						"          EffectiveStartDate,",
						"          EndDate,",
						"          CreatedDate",
						"     )) ~> PractiseCustomerTableExisting"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_pipelinestats_appendCSVfile')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_blob_dummyfile",
								"type": "DatasetReference"
							},
							"name": "DummySource"
						},
						{
							"dataset": {
								"referenceName": "ds_SrcDatalake_ADW_Folder",
								"type": "DatasetReference"
							},
							"name": "LogDEstinationFile"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_SrcDatalake_ADW_Folder",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						},
						{
							"name": "select1"
						},
						{
							"name": "union1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     ADFName as string,",
						"     PipelineName as string,",
						"     TriggerName as string,",
						"     TriggerTime as string,",
						"     FileName as string",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> DummySource",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> LogDEstinationFile",
						"DummySource derive(ADFName = $ADFName,",
						"          PipelineName = $PipelineName,",
						"          TriggerName = $TriggerName,",
						"          TriggerTime = $TriggerTime,",
						"     partitionBy('hash', 1)) ~> derivedColumn1",
						"derivedColumn1 select(mapColumn(",
						"          ADFName,",
						"          PipelineName,",
						"          TriggerName,",
						"          TriggerTime",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1, LogDEstinationFile union(byName: true)~> union1",
						"union1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:[($FileName)],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_pipelinestats_newCSVfile')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_blob_dummyfile",
								"type": "DatasetReference"
							},
							"name": "Dummy"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_SrcDatalake_ADW_Folder",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     ADFName as string,",
						"     PipelineName as string,",
						"     TriggerName as string,",
						"     TriggerTime as string,",
						"     FileName as string",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Dummy",
						"Dummy derive(ADFName = $ADFName,",
						"          PipelineName = $PipelineName,",
						"          TriggerName = $TriggerName,",
						"          TriggerTime = $TriggerTime) ~> derivedColumn1",
						"derivedColumn1 select(mapColumn(",
						"          ADFName,",
						"          PipelineName,",
						"          TriggerName,",
						"          TriggerTime",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:[($FileName)],",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_separate_error_records')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This data flow separates the error records to separate file.",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_src_blob_custoreerrorfile",
								"type": "DatasetReference"
							},
							"name": "ADWCustomer"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_practisedb_customer_error",
								"type": "DatasetReference"
							},
							"name": "PractiseCustomerError"
						},
						{
							"dataset": {
								"referenceName": "ds_PractiseDB_Customer",
								"type": "DatasetReference"
							},
							"name": "PractiseCustomer"
						}
					],
					"transformations": [
						{
							"name": "ConditionCheck"
						},
						{
							"name": "derivedColumn1"
						},
						{
							"name": "derivedColumn2"
						}
					],
					"scriptLines": [
						"parameters{",
						"     InputFile as string ('Customer_error.txt')",
						"}",
						"source(output(",
						"          CustomerID as string,",
						"          NameStyle as string,",
						"          Title as string,",
						"          FirstName as string,",
						"          MiddleName as string,",
						"          LastName as string,",
						"          Suffix as string,",
						"          CompanyName as string,",
						"          SalesPerson as string,",
						"          EmailAddress as string,",
						"          Phone as string,",
						"          PasswordHash as string,",
						"          PasswordSalt as string,",
						"          rowguid as string,",
						"          ModifiedDate as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> ADWCustomer",
						"ADWCustomer split(isNull(toInteger(CustomerID)),",
						"     disjoint: false) ~> ConditionCheck@(ErrorRecords, GoodRecords)",
						"ConditionCheck@ErrorRecords derive(InputFileName = $InputFile) ~> derivedColumn1",
						"ConditionCheck@GoodRecords derive(InputFileName = $InputFile) ~> derivedColumn2",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     preSQLs:['TRUNCATE TABLE Customer_error'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          CustomerID,",
						"          NameStyle,",
						"          Title,",
						"          FirstName,",
						"          MiddleName,",
						"          LastName,",
						"          Suffix,",
						"          CompanyName,",
						"          SalesPerson,",
						"          EmailAddress,",
						"          Phone,",
						"          PasswordHash,",
						"          PasswordSalt,",
						"          rowguid,",
						"          ModifiedDate,",
						"          FileName = InputFileName",
						"     )) ~> PractiseCustomerError",
						"derivedColumn2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          CustomerID as integer,",
						"          FirstName as string,",
						"          LastName as string,",
						"          CompanyName as string,",
						"          SalesPerson as string,",
						"          EmailAddress as string,",
						"          IsActive as boolean,",
						"          EffectiveStartDate as timestamp,",
						"          EndDate as timestamp,",
						"          CreatedDate as timestamp",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     preSQLs:['TRUNCATE TABLE Customer'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          CustomerID,",
						"          NameStyle,",
						"          Title,",
						"          FirstName,",
						"          MiddleName,",
						"          LastName,",
						"          Suffix,",
						"          CompanyName,",
						"          SalesPerson,",
						"          EmailAddress,",
						"          Phone,",
						"          PasswordHash,",
						"          PasswordSalt,",
						"          rowguid,",
						"          ModifiedDate,",
						"          FileName = InputFileName",
						"     )) ~> PractiseCustomer"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_split_bigFile_ToMultipleSmallFiles')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Split the source file by country ",
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "ls_mission100Datalake",
								"type": "LinkedServiceReference"
							},
							"name": "PractiseDbCovidHospitalization"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_ADW_Stage",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          country as string,",
						"          indicator as string,",
						"          date as date,",
						"          year_week as string,",
						"          value as short,",
						"          source as string,",
						"          url as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     format: 'delimited',",
						"     fileSystem: 'inputdata',",
						"     folderPath: 'PractiseStage',",
						"     fileName: 'covidhospitalization.csv',",
						"     columnDelimiter: ',',",
						"     escapeChar: '\\\\',",
						"     quoteChar: '\\\"',",
						"     columnNamesAsHeader: true) ~> PractiseDbCovidHospitalization",
						"PractiseDbCovidHospitalization derive(country = 'PractiseStage/'+ country + '.csv') ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     rowUrlColumn:'country',",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/pipeline1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "GetBatches",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "SELECT distinct ((ROW_NUMBER()OVER(ORDER BY Date ASC)  - 1)/5000) AS BatchID\n  FROM [dbo].[covidhospitalization] ORDER BY BatchID",
								"queryTimeout": "01:52:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "ds_PractiseDB_Customer",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/PL_Error_handling')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Divert the error records to separate table using data flow",
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_separate_error_records",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"ADWCustomer": {
										"FileName": "Customer_error.txt"
									},
									"PractiseCustomerError": {},
									"PractiseCustomer": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-09-02T21:38:35Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_separate_error_records')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_AutoIncrement_SurrogateKey')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_IncrementID_SurrogateKey",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"ProductOriginalFile": {},
									"ProductNewFile": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-09-03T04:46:08Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_PipelineProcessStats_CSV')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This pipeline writes stats to csv file. On daily basis, for first run it should create a new file with date.csv and next consecutive runs, it should append data.",
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					},
					{
						"name": "Get Metadata1",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "Set variable1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "ds_SrcDatalake_ADW_Folder",
								"type": "DatasetReference",
								"parameters": {
									"FileName": {
										"value": "@variables('File')",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"exists"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					},
					{
						"name": "Check if file exists",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Get Metadata1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@activity('Get Metadata1').output.exists",
								"type": "Expression"
							},
							"ifFalseActivities": [
								{
									"name": "New file creation",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "df_pipelinestats_newCSVfile",
											"type": "DataFlowReference",
											"parameters": {
												"ADFName": {
													"value": "'@{pipeline().DataFactory}'",
													"type": "Expression"
												},
												"PipelineName": {
													"value": "'@{pipeline().Pipeline}'",
													"type": "Expression"
												},
												"TriggerName": {
													"value": "'@{pipeline().TriggerName}'",
													"type": "Expression"
												},
												"TriggerTime": {
													"value": "'@{pipeline().TriggerTime}'",
													"type": "Expression"
												},
												"FileName": {
													"value": "'@{variables('File')}'",
													"type": "Expression"
												}
											},
											"datasetParameters": {
												"Dummy": {},
												"sink1": {
													"FileName": {
														"value": "@variables('File')",
														"type": "Expression"
													}
												}
											}
										},
										"staging": {},
										"compute": {
											"coreCount": 8,
											"computeType": "General"
										},
										"traceLevel": "Fine"
									}
								}
							],
							"ifTrueActivities": [
								{
									"name": "Append DF",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "df_pipelinestats_appendCSVfile",
											"type": "DataFlowReference",
											"parameters": {
												"ADFName": {
													"value": "'@{pipeline().DataFactory}'",
													"type": "Expression"
												},
												"PipelineName": {
													"value": "'@{pipeline().Pipeline}'",
													"type": "Expression"
												},
												"TriggerName": {
													"value": "'@{pipeline().TriggerName}'",
													"type": "Expression"
												},
												"TriggerTime": {
													"value": "'@{pipeline().TriggerTime}'",
													"type": "Expression"
												},
												"FileName": {
													"value": "'@{variables('File')}'",
													"type": "Expression"
												}
											},
											"datasetParameters": {
												"DummySource": {},
												"LogDEstinationFile": {
													"FileName": {
														"value": "@variables('File')",
														"type": "Expression"
													}
												},
												"sink1": {
													"FileName": {
														"value": "@variables('File')",
														"type": "Expression"
													}
												}
											}
										},
										"staging": {},
										"compute": {
											"coreCount": 8,
											"computeType": "General"
										},
										"traceLevel": "Fine"
									}
								}
							]
						}
					},
					{
						"name": "Set variable1",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Wait1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "File",
							"value": {
								"value": "@concat(formatDateTime(utcNow(),'yyyy-MM-dd'),'_log.csv')",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"File": {
						"type": "String"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-09-03T00:31:45Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_pipelinestats_newCSVfile')]",
				"[concat(variables('factoryId'), '/dataflows/df_pipelinestats_appendCSVfile')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_RunningTotal')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_RunningTotal",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"ProductCSV": {},
									"ProductSink": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-09-03T05:20:44Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_RunningTotal')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_SCD_Type1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This implement SCD Type#1 where records will update if exists else insert.",
				"activities": [
					{
						"name": "SCD Type1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_SCD_Type_1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"CustomerCSV": {},
									"PractisseDB": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-09-03T22:46:51Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_SCD_Type_1')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_SCD_Type2')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_SCD_Type_2",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"CustomerInputFile": {},
									"PractoseDBCustomer": {},
									"PractiseCustomerTable": {},
									"PractiseCustomerTableExisting": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-09-04T01:17:55Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_SCD_Type_2')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_split_Bigfile_byColumnName')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Split the covid file in to multiple files with country name as small file name",
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_split_bigFile_ToMultipleSmallFiles",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"PractiseDbCovidHospitalization": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_split_bigFile_ToMultipleSmallFiles')]"
			]
		}
	]
}